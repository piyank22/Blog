{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maclImdb\u001b[0m/                                   poincare.ipynb\r\n",
      "AttentionAllNeed.ipynb                     PyTorch.ipynb\r\n",
      "\u001b[01;34mattention-is-all-you-need-pytorch-master\u001b[0m/  question-words.txt\r\n",
      "\u001b[01;34mcifar\u001b[0m/                                     question-words.txt~\r\n",
      "\u001b[01;34mCS231n\u001b[0m/                                    random.ipynb\r\n",
      "\u001b[01;34mData_law\u001b[0m/                                  senteval.ipynb\r\n",
      "DenseNet.ipynb                             sonnet_tf_dnc.ipynb\r\n",
      "doc2vec_imdb.ipynb                         super_resolution_with_caffe2.ipynb\r\n",
      "doc2vec.ipynb                              \u001b[01;34mtempdata\u001b[0m/\r\n",
      "\u001b[01;34mDrQA\u001b[0m/                                      \u001b[01;34mtmp\u001b[0m/\r\n",
      "\u001b[01;34mfaces\u001b[0m/                                     train.txt\r\n",
      "\u001b[01;34mFAST.ai_TF\u001b[0m/                                train-v1.1.json\r\n",
      "Fconv seq2seq.ipynb                        \u001b[01;34mUSC\u001b[0m/\r\n",
      "\u001b[01;34mgensim_Trained models\u001b[0m/                     u_s_code_scrapper.ipynb\r\n",
      "\u001b[01;34mhtm_uscAll@115-95not91\u001b[0m/                    \u001b[01;34mUSCpickle\u001b[0m/\r\n",
      "\u001b[01;34mInferSent-master\u001b[0m/                          word2vec.ipynb\r\n",
      "infersent-pani.ipynb                       \u001b[01;34mwork\u001b[0m/\r\n",
      "\u001b[01;34mnew_legal_data\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piyank/Documents/Python_Scripts/InferSent-master/encoder\n"
     ]
    }
   ],
   "source": [
    "% cd InferSent-master/encoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo.ipynb          infersent.allnli.pickle  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  samples.txt\r\n",
      "\u001b[01;32mevaluate_model.py\u001b[0m*  models.py                README.md     xutils.py\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "import matplotlib\n",
    "\n",
    "import numpy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "GLOVE_PATH = '../dataset/GloVe/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyank/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BLSTMEncoder(\n",
       "  (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('infersent.allnli.pickle')\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_glove_path(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9815\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "with open('samples.txt') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip())\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentences = sentences[1000:1400]\n",
    "sentences = sentences[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = numpy.random.randint(0,high=2,size=(300))\n",
    "test_answers = numpy.random.randint(0,high=2,size=(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2218(/2238) words with glove vectors\n",
      "Vocab size : 2218\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyank/Documents/Python_Scripts/InferSent-master/encoder/models.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n"
     ]
    }
   ],
   "source": [
    "sent_encoded = model.encode( sentences )\n",
    "test_encoded_sent = model.encode( test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 4096)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.10451625,  0.09512675,  0.        , ...,  0.05411991,\n",
       "         0.        ,  0.03131687], dtype=float32),\n",
       " array([ 0.07801577,  0.11585563,  0.0873036 , ...,  0.00039723,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " array([ 0.0600462 ,  0.08212361,  0.03758186, ..., -0.02280346,\n",
       "        -0.03814263, -0.0099379 ], dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded[0] , sent_encoded[1] , sent_encoded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 8192)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded = sent_encoded.reshape( sent_encoded.shape[0]//2 , -1 )\n",
    "test_encoded_sent = test_encoded_sent.reshape( test_encoded_sent.shape[0]//2 , -1 )\n",
    "sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.10451625,  0.09512675,  0.        , ...,  0.00039723,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " array([ 0.0600462 ,  0.08212361,  0.03758186, ...,  0.17117991,\n",
       "         0.        ,  0.01631646], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded[0] , sent_encoded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable( torch.Tensor(sent_encoded) )\n",
    "x_test = Variable( torch.Tensor(test_encoded_sent) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Variable( torch.LongTensor(answers) )\n",
    "y_test = Variable( torch.LongTensor(test_answers) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n1 = nn.Linear(8192,1024)\n",
    "d1 = nn.Dropout(p=.5)\n",
    "n2 = nn.Linear(1024,64)\n",
    "d2 = nn.Dropout(p=.9)\n",
    "n3 = nn.Linear(64,2)\n",
    "\n",
    "final_layer = nn.Sequential( n1 , d1 , n2 , d2 , n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam( final_layer.parameters() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.691713273525238\n",
      "1 0.82391756772995\n",
      "2 0.8211984038352966\n",
      "3 0.7211378812789917\n",
      "4 0.8132596015930176\n",
      "5 0.8302390575408936\n",
      "6 0.7096949219703674\n",
      "7 0.7008435726165771\n",
      "8 0.7642484307289124\n",
      "9 0.7264686822891235\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    \n",
    "    y_pred = final_layer(x)\n",
    "\n",
    "    iter_loss = loss(y_pred, y)\n",
    "    print(t, iter_loss.data[0])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    iter_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_layer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = final_layer( x_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2685 -1.8548\n",
       " 0.6488  0.0236\n",
       "-0.7314 -0.4306\n",
       "-0.5573  0.6731\n",
       "-0.2402  0.3275\n",
       "-0.2192 -0.5896\n",
       " 0.1613 -0.5472\n",
       " 0.5360 -0.3332\n",
       " 0.2002 -0.2063\n",
       " 0.3114  0.3549\n",
       " 0.8878 -0.8207\n",
       " 0.2412  0.0715\n",
       "-0.5392 -0.1468\n",
       "-0.8140  0.5147\n",
       " 0.2565 -0.1589\n",
       " 0.8424 -1.0237\n",
       "-1.2985 -0.4017\n",
       " 1.1588 -2.3381\n",
       " 0.0228  0.0656\n",
       " 0.1925  0.2493\n",
       " 0.6097  0.5486\n",
       "-0.2717 -0.3972\n",
       " 0.0107 -0.1517\n",
       " 0.9946 -0.2448\n",
       "-0.2546 -0.0108\n",
       " 0.3629 -0.0986\n",
       "-0.4098 -1.4859\n",
       "-0.4816 -0.3192\n",
       " 1.2830 -0.6739\n",
       " 0.3408  0.1184\n",
       " 0.7535 -0.4104\n",
       " 0.5120 -0.0410\n",
       " 0.0818  0.0890\n",
       "-0.1886  0.1968\n",
       " 0.2168 -0.0698\n",
       " 0.3169 -0.2603\n",
       "-0.3766  0.5728\n",
       "-0.9126 -0.8264\n",
       "-0.1063 -0.7426\n",
       " 0.1484  0.2046\n",
       " 0.0463  0.1727\n",
       "-0.4809 -0.1730\n",
       " 0.7052  0.3047\n",
       " 0.8051  0.2020\n",
       "-0.4499  0.9349\n",
       " 1.5366  0.7270\n",
       "-0.6114 -0.8298\n",
       "-0.4777 -0.5626\n",
       " 0.1800 -0.4327\n",
       " 0.2315 -0.3974\n",
       " 0.1138  0.7910\n",
       "-0.4632 -0.5557\n",
       " 0.4749 -0.2313\n",
       "-0.7741  0.2969\n",
       " 1.2859  0.9043\n",
       "-0.3306 -0.7061\n",
       "-0.4783  0.3862\n",
       "-0.1450 -0.0526\n",
       " 1.0087  1.2582\n",
       "-2.1939 -1.0056\n",
       " 0.7716  0.9233\n",
       "-0.6444  0.7672\n",
       "-0.0813  0.0578\n",
       "-0.0176 -0.5376\n",
       "-1.0434 -0.2082\n",
       "-0.5644  1.0194\n",
       " 0.0270 -0.5807\n",
       " 0.2960 -0.2253\n",
       " 0.4609  0.1454\n",
       " 0.4460 -0.0440\n",
       "-0.4919 -0.1678\n",
       "-0.0863 -0.1961\n",
       "-1.6209 -0.4516\n",
       " 2.1379  1.0108\n",
       " 0.8210 -0.0086\n",
       " 1.1643 -0.8624\n",
       " 0.8035  0.5592\n",
       "-0.2021  0.3232\n",
       "-0.1273 -0.2780\n",
       "-0.2050  0.0632\n",
       " 1.4510 -0.0572\n",
       "-0.1713 -0.0654\n",
       "-0.5215  0.0716\n",
       " 1.8995 -1.0772\n",
       "-0.0921  0.0576\n",
       " 0.1519  0.1606\n",
       "-0.3033 -0.5570\n",
       "-0.9789 -0.3235\n",
       "-0.2041  0.3521\n",
       " 0.0347  0.2637\n",
       " 0.3142  0.6454\n",
       "-0.1336 -0.5452\n",
       "-0.7057 -0.1634\n",
       " 0.4779 -0.4016\n",
       " 0.7631 -1.3264\n",
       " 0.4893  0.1795\n",
       "-0.1313  0.6152\n",
       "-0.0680 -0.7194\n",
       " 0.3660  0.0683\n",
       " 0.2230  0.3618\n",
       " 0.7420 -0.4361\n",
       " 0.0882 -0.1078\n",
       " 0.5156  0.1842\n",
       " 0.7724  0.0825\n",
       "-0.3547 -0.1766\n",
       "-0.1255  0.2889\n",
       " 0.1075 -1.4724\n",
       "-0.4648  0.0769\n",
       "-0.1725 -0.4290\n",
       "-1.7935 -1.1251\n",
       "-0.0527  0.2223\n",
       " 0.0523  0.3575\n",
       "-0.4574 -0.7462\n",
       " 0.1792  0.2237\n",
       " 0.3386 -0.2768\n",
       "-0.3597  0.0514\n",
       " 0.4043 -0.1880\n",
       " 0.2088  0.4789\n",
       " 0.1755  0.1322\n",
       "-0.1439 -0.8753\n",
       "-0.3278 -0.3635\n",
       "-0.5250 -0.6169\n",
       "-1.3778  0.2359\n",
       " 0.6838 -0.9139\n",
       " 1.1050  0.0088\n",
       " 0.2241 -0.4730\n",
       " 0.4575  0.1255\n",
       " 1.2136  0.1226\n",
       " 0.6285  0.3860\n",
       "-0.7653  0.2609\n",
       " 0.5292  0.0260\n",
       "-0.7072 -0.4089\n",
       "-0.3096  0.3405\n",
       " 0.8692 -0.0141\n",
       "-0.5302 -0.6792\n",
       "-1.5458  0.1852\n",
       "-1.3980 -0.0512\n",
       "-0.9929  0.0042\n",
       "-0.8672 -1.0954\n",
       " 0.1537 -0.5136\n",
       " 1.1052  0.6516\n",
       " 1.1443 -1.7788\n",
       " 0.3402  0.3672\n",
       "-0.0675 -1.0987\n",
       " 0.4705 -0.4602\n",
       " 2.3492  0.0224\n",
       " 1.9216  0.5859\n",
       " 0.2726  0.0238\n",
       "-0.4997  0.0323\n",
       "-0.3111 -0.0679\n",
       " 0.2173 -0.1854\n",
       " 0.0291 -0.3317\n",
       " 0.2958 -0.3487\n",
       " 0.7355  0.9383\n",
       "-0.0042 -0.4497\n",
       " 1.3041 -0.4129\n",
       " 1.0555 -0.0687\n",
       " 0.4255 -0.1459\n",
       " 0.8493 -0.5369\n",
       " 0.3570 -0.6366\n",
       " 1.3463  0.1709\n",
       "-1.0149 -0.8219\n",
       " 0.2865  0.3289\n",
       "-1.5259 -0.3802\n",
       " 0.6031 -1.2266\n",
       "-1.0944 -0.9408\n",
       " 0.0669 -0.4088\n",
       "-0.6165 -0.1707\n",
       "-0.3097 -0.4256\n",
       " 1.1058 -0.0158\n",
       " 0.5030 -1.3008\n",
       "-0.3667  0.5585\n",
       " 0.1344 -0.8747\n",
       "-0.2618 -1.2505\n",
       " 0.7553 -0.3896\n",
       " 1.2322  0.9771\n",
       "-0.1917  0.3513\n",
       " 0.0641 -0.4281\n",
       " 1.0419 -0.5985\n",
       " 0.6203  0.3554\n",
       " 2.6285  0.2163\n",
       "-0.1412 -0.9006\n",
       "-0.4836 -0.2662\n",
       " 1.5248 -2.4791\n",
       "-1.5774 -1.0567\n",
       " 0.1416  0.0994\n",
       " 1.2370  1.3809\n",
       " 0.0359 -0.3731\n",
       " 0.2579  0.0221\n",
       "-0.1008 -0.2813\n",
       "-0.5452 -1.6310\n",
       " 0.2678 -0.6336\n",
       " 0.5232  0.3258\n",
       " 0.4919 -0.5693\n",
       " 0.3108 -0.0200\n",
       " 0.2329  0.1870\n",
       " 0.2749  0.4131\n",
       "-0.1744 -0.1494\n",
       " 0.3844 -0.2627\n",
       " 0.6389 -1.1740\n",
       "[torch.FloatTensor of size 200x2]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyank/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "prob , index = F.softmax(y_test_pred).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_test = prob.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_test = index.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.89314246,  0.6513961 ,  0.57463825,  0.77389717,  0.6382336 ,\n",
       "         0.59155178,  0.67007071,  0.70458615,  0.60024893,  0.51088142,\n",
       "         0.8466388 ,  0.54230809,  0.59687167,  0.79061085,  0.60238099,\n",
       "         0.86600816,  0.71029383,  0.97060132,  0.51069236,  0.51420814,\n",
       "         0.51528132,  0.53132898,  0.54051113,  0.77547348,  0.56065977,\n",
       "         0.61336708,  0.74574679,  0.54052144,  0.87620002,  0.55538356,\n",
       "         0.7620346 ,  0.63483042,  0.50179374,  0.59516323,  0.57115638,\n",
       "         0.64041734,  0.72099698,  0.52152824,  0.65391505,  0.51404333,\n",
       "         0.53155708,  0.57636327,  0.59881335,  0.64635628,  0.79975593,\n",
       "         0.69202054,  0.55439317,  0.5212031 ,  0.64855433,  0.65223676,\n",
       "         0.6631068 ,  0.52310801,  0.66955554,  0.74479157,  0.59425175,\n",
       "         0.59278649,  0.70361406,  0.52310169,  0.56206113,  0.76644379,\n",
       "         0.53785598,  0.80402774,  0.53470904,  0.62716037,  0.69745559,\n",
       "         0.82974154,  0.64740229,  0.6274485 ,  0.57822472,  0.62011057,\n",
       "         0.58032483,  0.52740753,  0.76301634,  0.75531656,  0.69628733,\n",
       "         0.88357723,  0.56077206,  0.62839663,  0.53759825,  0.56665212,\n",
       "         0.81879747,  0.5264554 ,  0.64408296,  0.95151055,  0.53736401,\n",
       "         0.50216353,  0.56309921,  0.6582197 ,  0.63558936,  0.5570156 ,\n",
       "         0.58205062,  0.60145718,  0.63234442,  0.70671946,  0.88987601,\n",
       "         0.5768227 ,  0.6784156 ,  0.65732509,  0.57388633,  0.53464657,\n",
       "         0.76460189,  0.54884797,  0.58211368,  0.66595823,  0.54442996,\n",
       "         0.60214359,  0.82919163,  0.63221139,  0.56376958,  0.66113192,\n",
       "         0.56831348,  0.57569712,  0.57169771,  0.51111716,  0.64918703,\n",
       "         0.60135096,  0.64390355,  0.56711859,  0.51082397,  0.67511779,\n",
       "         0.50893033,  0.52295035,  0.83393502,  0.83169729,  0.7495476 ,\n",
       "         0.66754156,  0.5822497 ,  0.74856985,  0.56032467,  0.73618889,\n",
       "         0.6232    ,  0.57402331,  0.65703475,  0.70749378,  0.53717911,\n",
       "         0.84953535,  0.7936058 ,  0.7304945 ,  0.55679786,  0.66089916,\n",
       "         0.611498  ,  0.94897825,  0.50673527,  0.7371428 ,  0.71721947,\n",
       "         0.91107595,  0.79177356,  0.56188011,  0.62995219,  0.56050754,\n",
       "         0.59933382,  0.58922178,  0.65576279,  0.55052948,  0.60957372,\n",
       "         0.84774369,  0.75476855,  0.63909638,  0.79997122,  0.72979432,\n",
       "         0.76413453,  0.54811692,  0.51060057,  0.75872785,  0.8617202 ,\n",
       "         0.53831869,  0.61672306,  0.60964131,  0.5289461 ,  0.75429106,\n",
       "         0.85860479,  0.71610069,  0.73283857,  0.72882402,  0.75857103,\n",
       "         0.56342363,  0.6325202 ,  0.6206367 ,  0.83758777,  0.56584001,\n",
       "         0.91775262,  0.68122858,  0.55413991,  0.98208207,  0.62729949,\n",
       "         0.51056218,  0.53591931,  0.60084468,  0.55869228,  0.54501659,\n",
       "         0.74757606,  0.7112357 ,  0.54918641,  0.74291527,  0.5819506 ,\n",
       "         0.51148409,  0.53448725,  0.50624377,  0.6563369 ,  0.85970944], dtype=float32),\n",
       " array([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_test,index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
